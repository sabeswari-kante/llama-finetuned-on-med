{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b915dc-b1be-4153-b19b-5e96cd039ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb8bec-e6e3-4db2-be45-57d4d4d9a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "\"\"\"\n",
    "case template = \n",
    "    problem : \n",
    "        age, cheif complaints, histories,\n",
    "        examinalfindings,\n",
    "        reasoning, differentials, investigations, \n",
    "        refferals, \n",
    "        variables--------------days, temp,years, bp, risks,\n",
    "\n",
    "ADVERSARIAL (TRICKY) CASES TEMPLATES=\n",
    "    diagnosis\n",
    "    mimics\n",
    "    input \n",
    "    reasoning\n",
    "    plan\n",
    "\n",
    "inputs : patient age, complaint,histories, examinal findings\n",
    "\n",
    "outputs : reasoining,diff, investigations, referrals/ plan\n",
    "\n",
    "dataset- instruction : act a med ai asistant, diagnose and plan\n",
    "            input: inputs\n",
    "            output : outputs\n",
    "\n",
    "\"\"\"    \n",
    "# same for SURGICAL PATCH DATA GENERATOR(Focus: Fixes Dengue/Malaria and Chickenpox/Scabies confusion_) \n",
    "# and \n",
    "#  TROPICAL / INFECTIOUS - instruction data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5dd377-67ee-4ab9-b72f-648a72e438de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27992ad7-9d5c-47ed-a517-4e8532ed0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAST INSTALL (Pre-compiled)\n",
    "# Run this ONLY if the previous install is taking forever\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e5f21d-83a5-4fa4-9366-becb1e938a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force install the latest PEFT to fix the \"ensure_weight_tying\" error\n",
    "!pip install --upgrade --force-reinstall \"peft @ git+https://github.com/huggingface/peft.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17981228-c921-436e-be84-95cff1202d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Force-align Torch and Torchvision\n",
    "# We strictly request the version Unsloth is asking for (>=0.24.0)\n",
    "!pip install --upgrade \"torch==2.9.1\" \"torchvision>=0.24.0\" \"torchaudio>=2.9.0\"\n",
    "\n",
    "# 2. Re-install Unsloth (Just to be safe after the torch update)\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf89e5-ad93-4287-bab2-569f3cc01b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "# Force Single GPU Mode\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a052b0-a3a7-4da0-9d6e-4750025c1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LOAD MODEL\n",
    "max_seq_length = 2048 \n",
    "print(\"‚¨áÔ∏è Loading Llama-3.1-8B (Senior Resident)...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a7f37-35d0-45d9-adf5-882c1ec6587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. CONFIGURE ADAPTERS\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9e2792-8344-406a-9fdf-eb31782a1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DEFINE ROBUST FORMATTING FUNCTION (The Fix)\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    # Case 1: Batch Mode (List of strings) - The Trainer uses this during training\n",
    "    if isinstance(examples[\"instruction\"], list):\n",
    "        instructions = examples[\"instruction\"]\n",
    "        inputs       = examples[\"input\"]\n",
    "        outputs      = examples[\"output\"]\n",
    "        texts = []\n",
    "        for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "            text = alpaca_prompt.format(instruction, input, output) + tokenizer.eos_token\n",
    "            texts.append(text)\n",
    "        return texts # <--- Returns a LIST (Correct for Trainer)\n",
    "\n",
    "    # Case 2: Single Example Mode - Unsloth uses this for the safety check\n",
    "    else:\n",
    "        instruction = examples[\"instruction\"]\n",
    "        input       = examples[\"input\"]\n",
    "        output      = examples[\"output\"]\n",
    "        text = alpaca_prompt.format(instruction, input, output) + tokenizer.eos_token\n",
    "        return [text] # <--- Returns a LIST of 1 string\n",
    "\n",
    "print(\"üìÇ Loading 'FINAL_MASTER_DATASET.json'...\")\n",
    "dataset = load_dataset(\"json\", data_files=\"FINAL_MASTER_DATASET.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e5116-a008-404f-a232-0d9fe2caee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Train/Test\n",
    "dataset = dataset.train_test_split(test_size=0.05) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987c877-3407-42e8-bad0-d9f7687ad982",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìä Final Training on {len(dataset['train'])} cases\")\n",
    "print(f\"üìä Evaluating on {len(dataset['test'])} cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eb422e-6b5b-4a44-9b67-48ae2dfcdd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. TRAINING ARGUMENTS\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"test\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    \n",
    "    # --- Pass the Function Here (No manual mapping needed) ---\n",
    "    formatting_func = formatting_prompts_func, \n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 120,        \n",
    "        learning_rate = 2e-5,   \n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        output_dir = \"medical_llama_final_v1\",\n",
    "        \n",
    "        # Validation Settings\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 10,\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 20,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting Final Training...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a9939-d999-47dc-af38-82626028eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "print(\"‚úÖ FINAL MODEL READY.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b59d0b1-4951-4365-8e59-4702137d3022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# üöë RESCUE MISSION: RESUME TRAINING FROM CHECKPOINT-100\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. LOAD EVERYTHING AGAIN\n",
    "max_seq_length = 2048 \n",
    "print(\"‚¨áÔ∏è Reloading Model for Rescue...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "# 2. LOAD DATA\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    if isinstance(examples[\"instruction\"], list):\n",
    "        instructions = examples[\"instruction\"]\n",
    "        inputs       = examples[\"input\"]\n",
    "        outputs      = examples[\"output\"]\n",
    "        texts = []\n",
    "        for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "            text = alpaca_prompt.format(instruction, input, output) + tokenizer.eos_token\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "    else:\n",
    "        text = alpaca_prompt.format(examples[\"instruction\"], examples[\"input\"], examples[\"output\"]) + tokenizer.eos_token\n",
    "        return [text]\n",
    "\n",
    "print(\"üìÇ Reloading Dataset...\")\n",
    "dataset = load_dataset(\"json\", data_files=\"FINAL_MASTER_DATASET.json\", split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.05) \n",
    "\n",
    "# 3. CONFIGURE TRAINER\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"test\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    formatting_func = formatting_prompts_func,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 120,        \n",
    "        learning_rate = 2e-5,   \n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        output_dir = \"medical_llama_final_v1\", # <--- Points to where your checkpoint is\n",
    "        \n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 10,\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 20,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 4. THE MAGIC COMMAND\n",
    "# This checks your folder, finds 'checkpoint-100', and resumes instantly.\n",
    "print(\"üöÄ Resuming Training from Step 100...\")\n",
    "trainer.train(resume_from_checkpoint = True) \n",
    "\n",
    "print(\"‚úÖ RESCUE COMPLETE! Model finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9b831f-04f0-41b1-a743-d93b5c70675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Extract Data Automagically\n",
    "# When resuming, this loads the old history from the checkpoint + the new steps\n",
    "history = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# 2. Clean & Merge Data\n",
    "train_loss = history[history['loss'].notna()][['step', 'loss']].rename(columns={'loss': 'Training Loss'})\n",
    "val_loss = history[history['eval_loss'].notna()][['step', 'eval_loss']].rename(columns={'eval_loss': 'Validation Loss'})\n",
    "df = pd.merge(train_loss, val_loss, on='step', how='outer')\n",
    "\n",
    "# 2.5 SAFETY TWEAK: Remove duplicates if any exist\n",
    "df = df.drop_duplicates(subset=['step']).sort_values('step')\n",
    "\n",
    "# 3. Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set_style(\"white\")\n",
    "sns.despine()\n",
    "\n",
    "sns.lineplot(x=\"step\", y=\"Training Loss\", data=df, color='#FF9F1C', linewidth=3, label='Training Loss')\n",
    "sns.lineplot(x=\"step\", y=\"Validation Loss\", data=df, color='#5C9EAD', linewidth=3, label='Validation Loss')\n",
    "\n",
    "plt.title(\"Final Model Training Convergence (Resumed)\", fontsize=16, weight='bold', pad=20, color='#333333')\n",
    "plt.xlabel(\"Steps\", fontsize=14, labelpad=10)\n",
    "plt.ylabel(\"Loss\", fontsize=14, labelpad=10)\n",
    "plt.legend(frameon=False, fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the final score\n",
    "if not val_loss.empty:\n",
    "    final_val = df['Validation Loss'].iloc[-1]\n",
    "    print(f\"üèÜ Final Validation Loss: {final_val:.4f} (Lower is better!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03ec73f-d2a2-435b-a5fa-8bc341184817",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Switch to Inference Mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def ask_doctor(input_text):\n",
    "    prompt = alpaca_prompt.format(\n",
    "        \"Act as a Medical Consultant. Diagnose and Plan.\",\n",
    "        input_text,\n",
    "        \"\"\n",
    "    )\n",
    "    inputs = tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True)\n",
    "    return tokenizer.batch_decode(outputs)[0].split(\"### Response:\\n\")[-1].replace(tokenizer.eos_token, \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4dde3d-c67a-4ec4-88b7-8953d5f1b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PART A: 5 SEEN CASES (Memory Check) ---\n",
    "print(\"üîé PART A: MEMORY CHECK (5 Random Seen Cases)\")\n",
    "print(\"=\"*60)\n",
    "with open('FINAL_MASTER_DATASET.json', 'r') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "# Pick 5 random cases\n",
    "seen_indices = random.sample(range(len(training_data)), 5)\n",
    "\n",
    "for i, idx in enumerate(seen_indices):\n",
    "    case = training_data[idx]\n",
    "    print(f\"\\nüìù SEEN CASE {i+1}:\")\n",
    "    print(f\"INPUT: {case['input'].splitlines()[1]}\") # Complaint\n",
    "    print(\"-\" * 20)\n",
    "    ai_response = ask_doctor(case['input'])\n",
    "    print(f\"ü§ñ AI SAYS:\\n{ai_response.splitlines()[0]}\") # Diagnosis line\n",
    "    print(f\"‚úÖ TRUTH:   {case['output'].splitlines()[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c35f4eb-0f3f-4fbb-9524-15ee29a4ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- PART B: 10 UNSEEN CASES (Intelligence Check) ---\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"üåç PART B: THE GAUNTLET (10 Unseen Conditions)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "unseen_exam = [\n",
    "    # 1. Endocrine\n",
    "    {\"name\": \"Hypothyroidism\", \"input\": \"Patient: 45y female\\nComplaint: weight gain and fatigue\\nHistory: Gained 10kg in 3 months despite poor appetite. Feels cold all the time. Constipation.\\nExam: HR 58 (Bradycardia). Dry skin. Delayed relaxation of deep tendon reflexes.\"},\n",
    "    # 2. Hematology\n",
    "    {\"name\": \"Iron Deficiency Anemia\", \"input\": \"Patient: 30y female\\nComplaint: feeling tired and dizzy\\nHistory: Heavy menstrual periods (menorrhagia). Craving ice (pica). Shortness of breath on exertion.\\nExam: Conjunctival pallor. Spoon-shaped nails (koilonychia). Tachycardia.\"},\n",
    "    # 3. Pediatrics (ENT)\n",
    "    {\"name\": \"Acute Otitis Media\", \"input\": \"Patient: 4y male\\nComplaint: crying and pulling at right ear\\nHistory: Had a cold 3 days ago. Now high fever and crying. Not eating.\\nExam: Temp 39C. Right tympanic membrane is red, bulging, and immobile.\"},\n",
    "    # 4. Dermatology\n",
    "    {\"name\": \"Cellulitis\", \"input\": \"Patient: 50y male\\nComplaint: red painful leg\\nHistory: Scratched leg in garden 2 days ago. Now lower leg is bright red, hot, and painful. Fevers.\\nExam: Erythema extending up the shin. Hot to touch. Tender. Inguinal lymph nodes tender.\"},\n",
    "    # 5. Cardiology\n",
    "    {\"name\": \"Atrial Fibrillation\", \"input\": \"Patient: 65y male\\nComplaint: palpitations\\nHistory: 'Heart feels like a fish flopping in chest'. Mild shortness of breath. History of hypertension.\\nExam: Pulse is irregularly irregular. BP 140/90. Chest clear.\"},\n",
    "    # 6. Respiratory\n",
    "    {\"name\": \"Acute Bronchiolitis (RSV)\", \"input\": \"Patient: 6 months female\\nComplaint: difficulty breathing and cough\\nHistory: Runny nose for 2 days. Now wheezing and working hard to breathe. Poor feeding.\\nExam: RR 50. Subcostal recession. Widespread wheeze and crackles.\"},\n",
    "    # 7. Neurology\n",
    "    {\"name\": \"Concussion (Mild TBI)\", \"input\": \"Patient: 20y male\\nComplaint: headache and confusion after hit\\nHistory: Hit head during rugby match. Brief loss of consciousness (<30s). Vomited once. Amnesia for the event.\\nExam: GCS 15. Pupils equal and reactive. No focal deficits.\"},\n",
    "    # 8. Urology\n",
    "    {\"name\": \"Benign Prostatic Hyperplasia (BPH)\", \"input\": \"Patient: 70y male\\nComplaint: waking up at night to pee\\nHistory: Frequency, urgency, and poor stream (dribbling). Nocturia x4. No pain.\\nExam: Abdomen soft. DRE: Smooth, enlarged, non-tender prostate.\"},\n",
    "    # 9. Allergy\n",
    "    {\"name\": \"Anaphylaxis\", \"input\": \"Patient: 18y female\\nComplaint: swollen lips and difficulty breathing\\nHistory: Ate peanuts 10 mins ago. Lips swelled up immediately. Wheezing. Feeling faint.\\nExam: Stridor audible. BP 80/50 (Hypotensive). Widespread hives (urticaria).\"},\n",
    "    # 10. MSK\n",
    "    {\"name\": \"Osteoarthritis (Knee)\", \"input\": \"Patient: 60y female\\nComplaint: right knee pain\\nHistory: Pain worse at end of day and after walking. Stiffness in morning <30 mins. No injury.\\nExam: Crepitus (crunching) on movement. Bony swelling. No warmth/redness.\"}\n",
    "]\n",
    "\n",
    "for i, case in enumerate(unseen_exam):\n",
    "    print(f\"\\nüß™ UNSEEN Q{i+1}: {case['name']}\")\n",
    "    print(f\"INPUT:\\n{case['input']}\")\n",
    "    print(\"-\" * 40)\n",
    "    ai_response = ask_doctor(case['input'])\n",
    "    \n",
    "    # Print the Diagnosis and the Reasoning to verify logic\n",
    "    print(\"ü§ñ AI REPORT:\")\n",
    "    print(ai_response)\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e32da9-f6fc-4fe1-af63-0e8a71794d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "print(\"üì¶ 1. Zipping your Medical AI Brain...\")\n",
    "# This compresses your fine-tuned weights into a zip file\n",
    "shutil.make_archive(\"medical_ai_brain\", 'zip', \"medical_llama_final_lora\")\n",
    "\n",
    "print(\"üì¶ 2. Checking for Dataset...\")\n",
    "if os.path.exists(\"FINAL_MASTER_DATASET.json\"):\n",
    "    print(\"‚úÖ Dataset found.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dataset not found! (Make sure you save it if you haven't already)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üö® DOWNLOAD THESE TWO FILES NOW:\")\n",
    "print(\"1. medical_ai_brain.zip (This is your trained model)\")\n",
    "print(\"2. FINAL_MASTER_DATASET.json (This is your data)\")\n",
    "print(\"=\"*60)\n",
    "print(\"üëâ KAGGLE: Check the 'Output' folder on the right sidebar.\")\n",
    "print(\"üëâ COLAB: Check the folder icon on the left sidebar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c7be7-ad24-4e05-b743-92ade1619bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# 1. Define the files we want\n",
    "files_to_download = [\"medical_ai_brain.zip\", \"FINAL_MASTER_DATASET.json\"]\n",
    "\n",
    "print(\"üëá CLICK THESE LINKS TO DOWNLOAD üëá\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "for filename in files_to_download:\n",
    "    if os.path.exists(filename):\n",
    "        # This generates a clickable blue link in Kaggle\n",
    "        display(FileLink(filename))\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Could not find {filename} (Did the previous zip script finish?)\")\n",
    "\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d008ef8-4b22-4c88-8b5e-024d7d1add04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üîÑ RELOAD & LAUNCH (Session Timeout Fix)\n",
    "# ==========================================\n",
    "\n",
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "import gradio as gr\n",
    "\n",
    "# 1. Point to your uploaded folder\n",
    "model_path = \"/kaggle/input/med-llama\" \n",
    "\n",
    "print(f\"üîÑ Waking up MedLlama from: {model_path}...\")\n",
    "\n",
    "# 2. RELOAD THE BRAIN (Fixes 'NameError: tokenizer not defined')\n",
    "try:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_path, \n",
    "        max_seq_length = 2048,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    print(\"‚úÖ Model & Tokenizer Reloaded Successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"‚ö†Ô∏è If this says 'Unsloth not defined', scroll up and run the pip install cell again!\")\n",
    "\n",
    "# 3. DEFINE THE DOC'S LOGIC (Stable Mode)\n",
    "def medical_consult(symptoms, history):\n",
    "    # Strict Prompt Formatting\n",
    "    user_input = f\"Complaint: {symptoms}\\nPatient Data: {history}\"\n",
    "    \n",
    "    prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Act as a Medical Consultant. Diagnose and Plan.\n",
    "\n",
    "### Input:\n",
    "{user_input}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Greedy Decoding (Stability > Creativity)\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens = 512, \n",
    "        use_cache = True,\n",
    "        do_sample = False,       # Strict mode\n",
    "        repetition_penalty = 1.2 \n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs)[0].split(\"### Response:\\n\")[-1].replace(tokenizer.eos_token, \"\")\n",
    "\n",
    "# 4. LAUNCH UI\n",
    "print(\"üöÄ Launching Interface...\")\n",
    "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"slate\")) as demo:\n",
    "    gr.Markdown(\"# üè• MedLlama\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            symptoms = gr.Textbox(label=\"Chief Complaint\")\n",
    "            history = gr.Textbox(label=\"Patient History + Exam\", lines=5)\n",
    "            submit_btn = gr.Button(\"Generate Consult\", variant=\"primary\")\n",
    "        with gr.Column(scale=1):\n",
    "            output = gr.Textbox(label=\"Report\", lines=15)\n",
    "    submit_btn.click(fn=medical_consult, inputs=[symptoms, history], outputs=output)\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
